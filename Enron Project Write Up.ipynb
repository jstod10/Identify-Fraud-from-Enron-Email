{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb1cded",
   "metadata": {},
   "source": [
    "### Project Goal\n",
    "\n",
    "The goal of this project was to examine data related to executives working at the infamous company Enron.  The dataset contains information on 146 individuals, with each record containing 21 features.  The purpose of the project was to utilize machine learning to determine which set of features could most effectively predict attributes a Person of Interest (POI) might display. 18 POIs have previously been identified\n",
    "\n",
    "Initial examination of the dataset revealed two troublesome records:\n",
    "\n",
    "* \"The Travel Agency in the Park\"\n",
    "* \"Total\"\n",
    "\n",
    "I removed those using the .pop() method, then created a for loop to iterate through the remaining entries to check for records with similar concerns and/or little to no data.  I ended up removing two more individual records.  The final dataset contained 142 entries. \n",
    "\n",
    "### Features Used\n",
    "\n",
    "The feature list I ended up using included the following:\n",
    "\n",
    "('poi', 'exercised_stock_options', 'total_stock_value', 'bonus', 'percent_poi_emails')\n",
    "\n",
    "The following is a list of features and the number of entries missing values for each:\n",
    "\n",
    "\n",
    "FEATURE                   COUNT\n",
    "-------------------------------\n",
    "deferral_payments         104  \n",
    "loan_advances             139  \n",
    "restricted_stock_deferred 126  \n",
    "deferred_income           95   \n",
    "exercised_stock_options   41   \n",
    "long_term_incentive       77   \n",
    "director_fees             127  \n",
    "to_messages               56   \n",
    "email_address             31   \n",
    "from_poi_to_this_person   56   \n",
    "from_messages             56   \n",
    "from_this_person_to_poi   56   \n",
    "shared_receipt_with_poi   56   \n",
    "salary                    48   \n",
    "total_payments            19   \n",
    "bonus                     61   \n",
    "expenses                  48   \n",
    "other                     51   \n",
    "restricted_stock          34   \n",
    "total_stock_value         17   \n",
    "\n",
    "\n",
    "#### Selection Process\n",
    "\n",
    "I started by removing the 'email_address' and 'other' features, as the information contained in those was irrelevant and/or undefined.  I tried to create a new feature 'percent_poi_emails' that combined the 'from_poi_to_this_person' and 'from_this_person_to_poi' features.  The purpose of this feature was to identify the percentage of emails connected to POIs.\n",
    "\n",
    "I used DecisionTreeClassifier first to evaluate different sets of features.  The feature list with the 2 features removed (as outlined above) proved to be the most accurate, although not quite to my liking.  I then utilized Select K Best to isolate the most important features fromt the dataset.  I utilized the results from that query to use the top five features using a minimum feature score of 10.0 as criteria.  \n",
    "\n",
    "Next, I used DecisionTreeClassifier to test feature sets using the top 4, 5, and 6 top features, along with my newly created feature to see which perfomed the best.  Using the top 4 features, along with my newly created feature, proved to be the best solution.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "I applied the following classifiers during testing and tuned the parameters for each manually:\n",
    "\n",
    "* SVC\n",
    "* DecisionTreeClassifier\n",
    "* NaiveBayes - Gaussian\n",
    "* AdaBoost\n",
    "* LogisticRegression\n",
    "\n",
    "Several test runs revealed DecisionTree as the top performer of the group\n",
    "\n",
    "Performance tests using DecisionTree and my selected feature set were as follows:\n",
    "\n",
    "* Accuracy: 0.81\n",
    "* Precision:  0.40\n",
    "* Recall: 0.50\n",
    "\n",
    "\n",
    "\n",
    "### Tuning\n",
    "\n",
    "The definition of tuning, according to [https://en.wikipedia.org/wiki/Hyperparameter_optimization#:~:text=In%20machine%20learning%2C%20hyperparameter%20optimization,typically%20node%20weights)%20are%20learned.]:\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
    "\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function\n",
    "takes a tuple of hyperparameters and returns the associated loss.  Cross-validation is often used to estimate this generalization performance.\n",
    "\n",
    "\n",
    "When tuning is not done well, it leads to ineffecient, inconsistent, and irrelevant results.  \n",
    "\n",
    "To tune the parameters, I created a for loop, incorporating GridSearchCV for parameter tuning to test different combination of classifiers with recommended parameters for each.  The following is a list of classifiers used, along with the tuned parameter settings for each:\n",
    "\n",
    "* GaussianNB - default\n",
    "\n",
    "* SVC - {'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "                             'cache_size': [7000],\n",
    "                             'tol': [0.0001, 0.001, 0.005, 0.05],\n",
    "                             'decision_function_shape': ['ovo', 'ovr'],\n",
    "                             'random_state': [randomState],\n",
    "                             'verbose' : [False],\n",
    "                             'C': [100, 1000, 10000]})\n",
    "                             \n",
    "* Decision Tree - {'criterion': ['gini', 'entropy'], \n",
    "                    'splitter': ['best', 'random'],                         \n",
    "                    'min_samples_split': [2, 10, 20],\n",
    "                    'max_depth': [None, 2, 4, 8, 16],\n",
    "                    'min_samples_leaf': [1, 3, 5, 7, 9],\n",
    "                    'max_leaf_nodes': [None, 6, 12, 24],\n",
    "                    'random_state': [randomState]})\n",
    "                    \n",
    "* AdaBoost - {'n_estimators': [25, 50, 100],\n",
    "              'algorithm': ['SAMME', 'SAMME.R'],\n",
    "              'learning_rate': [.2, .5, 1, 1.4, 2.],\n",
    "              'random_state': [randomState]})\n",
    "              \n",
    "\n",
    "* Logistic Regression - {'penalty': ['l1', 'l2'],\n",
    "                          'tol': [0.0001, 0.0005, 0.001, 0.005],\n",
    "                          'C': [1, 10, 100, 1000, 10000, 100000, 1000000],\n",
    "                          'fit_intercept': [True, False],\n",
    "                          'solver': ['liblinear'],\n",
    "                          'class_weight': [None, 'balanced'],\n",
    "                          'verbose': [False],\n",
    "                          'random_state': [randomState]})\n",
    "                            \n",
    " \n",
    "\n",
    "### Validation\n",
    "\n",
    "A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning modelâ€™s hyperparameters.  Validation also serves as a check on overfitting. \n",
    "\n",
    "When validation is done wrong, it can lead to innacurate findings.  A classic mistake in validation is when we use the same data for both training and testing.  It is important to train on a valid sample and then test on a different set of data to test the efficacy of chosen parameters.  It's important to find desirable symmetry between training and test sets, especially in regards to accuracy - the closer the two results, the more effective the chosen method. \n",
    "\n",
    "An important step in the validation process was to use feature scaling in an attempt to standardize the features, normalizing the data into a set range of minimum and maximum values using the MinMaxScaler.  This estimator scales and translates each feature individually into the given range on the training set.   \n",
    "I created a for loop to test different combinations of classifiers and the scaling method outlined above to isolate the best performers using cross validation.  I then ran several test runs and took note of the top 5 results and looked for consistency.\n",
    "\n",
    "Final validation was performed using the code detailed in the tester.py script,  which utilizes the Stratified Shuffle Split validation.  During this cross-validation process, the data is split into randomized training and testing.  Performance is then measured and compared between the two groups.  Symmetry between groups proves the effectiveness of the model.  \n",
    "\n",
    "### Testing\n",
    "\n",
    "3 metrics were used to evaluate different settings:\n",
    "\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "\n",
    "Accuracy is the liklihood of identifying a POI using the feature selection criteria I outlined above. \n",
    "\n",
    "Precision is the proportion of poisitive identifications that were actually correct. \n",
    "\n",
    "Recall is the proportion of actual positives that were identified correctly.\n",
    "\n",
    "Testing using the testing.py script revealed:\n",
    "\n",
    "* Accuracy of 0.78 \n",
    "\n",
    "* Precision and Recall came in at 0.30560 and 0.31400, respectively.\n",
    "\n",
    "These results are encouraging, as there is a nice balance between precision and recall, which proves the effectiveness of the model. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00bfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
